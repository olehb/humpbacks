{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw labels: [('0000e88ab.jpg', 'w_f48451c') ('0001f9222.jpg', 'w_c3d896a')\n",
      " ('00029d126.jpg', 'w_20df2c5') ('00050a15a.jpg', 'new_whale')\n",
      " ('0005c1ef8.jpg', 'new_whale') ('0006e997e.jpg', 'new_whale')\n",
      " ('000a6daec.jpg', 'w_dd88965') ('000f0f2bf.jpg', 'new_whale')\n",
      " ('0016b897a.jpg', 'w_64404ac') ('001c1ac5f.jpg', 'w_a6f9d33')]\n"
     ]
    }
   ],
   "source": [
    "# Loading raw training labels\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "raw_train_labels = np.loadtxt(os.path.join(DATA_FOLDER, 'train.csv'), skiprows=1, dtype=[('filename','U15'),('whale_id','U15')], delimiter=\",\")\n",
    "print('Raw labels:', raw_train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known labels: [('0000e88ab.jpg', 'w_f48451c') ('0001f9222.jpg', 'w_c3d896a')\n",
      " ('00029d126.jpg', 'w_20df2c5') ('000a6daec.jpg', 'w_dd88965')\n",
      " ('0016b897a.jpg', 'w_64404ac') ('001c1ac5f.jpg', 'w_a6f9d33')\n",
      " ('001cae55b.jpg', 'w_581ba42') ('00355ff28.jpg', 'w_cb622a2')\n",
      " ('00357e37a.jpg', 'w_d3b46e7') ('00442c882.jpg', 'w_8cad422')]\n"
     ]
    }
   ],
   "source": [
    "# Removing 'new_whale' from training set, so we can train only on known whales\n",
    "train_labels = raw_train_labels[raw_train_labels['whale_id'] != 'new_whale']\n",
    "print('Known labels:', train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 15697\n"
     ]
    }
   ],
   "source": [
    "# Loading train set filenames\n",
    "from glob import iglob\n",
    "\n",
    "files_with_known_whales = train_labels['filename'].tolist()\n",
    "train_filenames = [name.split(os.sep)[-1] for name in iglob(os.path.join(DATA_FOLDER, 'train', '*'))]\n",
    "train_filenames = [name for name in train_filenames if name in files_with_known_whales]\n",
    "\n",
    "assert len(train_filenames) == len(train_labels['filename'])\n",
    "\n",
    "print('# of training examples:', len(train_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to show images represented as numpy arrays\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def get_large_contour(filename):\n",
    "    \"\"\"\n",
    "    Returns the largest contour found in the image\n",
    "    @param filename: path to the image file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the image as-is\n",
    "    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    # Blur BGR images and convert to grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        img = cv2.GaussianBlur(img, (11, 11), 0)\n",
    "        # img = cv2.pyrMeanShiftFiltering(img, 51, 91)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Some fancy experiments with image thresholding...\n",
    "    # threshed_img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 5, 0)\n",
    "    # threshed_img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 2)\n",
    "    \n",
    "    # INV really helped a lot! Straight THRESH_BINARY or THRESH_TOZERO work much worse\n",
    "    # Without INV this function identifies object as being \"outside\" of the contour.\n",
    "    _, threshed_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # showarray(threshed_img)\n",
    "\n",
    "    # find contours\n",
    "    _, contours, _ = cv2.findContours(threshed_img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # This was used to find cotours which is not the entire image\n",
    "    # full_img_area = threshed_img.shape[0]*threshed_img.shape[1]\n",
    "\n",
    "    max_area = -1\n",
    "    max_contour = None\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_contour = c\n",
    "            \n",
    "    return max_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def crop_contour(img, contour, padding=10):\n",
    "    \"\"\"\n",
    "    Crops given image by bounding rectangle of a contour\n",
    "    \"\"\"\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    max_x = img.shape[1]\n",
    "    max_y = img.shape[0]    \n",
    "\n",
    "    img = img[max(0, y-padding):min(max_y, y+h+padding), max(0, x-padding):min(max_x, x+w+padding)]\n",
    "    # showarray(img)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "\n",
    "def get_input_vector(filename, resize_to_x, resize_to_y):\n",
    "    \"\"\"\n",
    "    This function prepares image data for machine learning operations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cropping largest object in the image...\n",
    "    contour = get_large_contour(filename)\n",
    "    img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "    img = crop_contour(img, contour)\n",
    "\n",
    "    # Resizing and normalizing...\n",
    "    img = cv2.resize(img, (resize_to_x, resize_to_y))\n",
    "    vector = normalize(img, norm='max', axis=1)    \n",
    "    \n",
    "    # Reshaping so it's stackable and digestable by keras\n",
    "    vector = np.expand_dims(vector, axis=2)\n",
    "    vector = np.expand_dims(vector, axis=0)\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(DATA_FOLDER, 'train', train_filenames[0])\n",
    "contour = get_large_contour(filename)\n",
    "x, y, w, h = cv2.boundingRect(contour)\n",
    "img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "img = cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 5)   \n",
    "img = cv2.drawContours(img, contour, -1, (0, 255, 0), 5)   \n",
    "showarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalculate_training_set = False\n",
    "\n",
    "if recalculate_training_set:\n",
    "    train_x_dim = 244\n",
    "    train_y_dim = 244\n",
    "    train_set = np.empty((len(train_filenames), train_x_dim, train_y_dim, 1), dtype=np.double)\n",
    "    for i in range(len(train_filenames)):\n",
    "        filename = os.path.join(DATA_FOLDER, 'train', train_filenames[i])\n",
    "        train_set[i] = get_input_vector(filename, train_x_dim, train_y_dim)\n",
    "    print(\"Saving training set...\")\n",
    "    np.save(os.path.join(DATA_FOLDER, 'cropped_tails.npy'), train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15697, 244, 244, 1)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = np.load(os.path.join(DATA_FOLDER, 'cropped_tails.npy'))\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_ordered = np.empty(train_labels.shape, dtype=train_labels.dtype)\n",
    "for i, train_filename in enumerate(train_filenames):\n",
    "    train_labels_ordered[i] = train_labels[train_labels['filename'] == train_filename]\n",
    "    \n",
    "assert train_filenames[:10] == train_labels_ordered['filename'].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 608  940 2316 ... 3070 4431 4502]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleg/.pyenv/versions/3.7.1/envs/humpback/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(train_labels_ordered['whale_id'])\n",
    "print(integer_encoded)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "\n",
    "assert onehot_encoded.shape == (len(train_labels_ordered), len(np.unique(train_labels_ordered['whale_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "train_set_size = int(math.floor(len(train_set) * 0.8))\n",
    "test_set_size = valid_set_size = (len(train_set) - train_set_size) // 2\n",
    "\n",
    "# TODO: Shuffle this first\n",
    "sub_train_set = train_set[:train_set_size]\n",
    "sub_train_labels = onehot_encoded[:train_set_size]\n",
    "valid_set = train_set[train_set_size:train_set_size+valid_set_size]\n",
    "valid_labels = onehot_encoded[train_set_size:train_set_size+valid_set_size]\n",
    "test_set = train_set[train_set_size+valid_set_size:]\n",
    "test_labels = onehot_encoded[train_set_size+valid_set_size:]\n",
    "\n",
    "assert len(sub_train_set) + len(valid_set) + len(test_set) == len(train_set) \n",
    "assert len(sub_train_labels) + len(valid_labels) + len(test_labels) == len(onehot_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 223, 223, 16)      80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 108, 108, 32)      8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 20, 20, 64)        131136    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         2097280   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5004)              645516    \n",
      "=================================================================\n",
      "Total params: 2,882,236\n",
      "Trainable params: 2,882,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "def get_model_fine_to_coarse(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32, kernel_size=4, padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=4))\n",
    "    model.add(Conv2D(filters=64, kernel_size=8, padding='valid', activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=8))\n",
    "    model.add(Conv2D(filters=128, kernel_size=16, padding='same', activation='relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    #model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = get_model_fine_to_coarse((224, 224, 1), num_classes=len(np.unique(train_labels_ordered['whale_id'])))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12557 samples, validate on 1570 samples\n",
      "Epoch 1/20\n",
      "12557/12557 [==============================] - 333s 26ms/step - loss: 8.3185 - acc: 0.0033 - val_loss: 8.2486 - val_acc: 0.0051\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.24858, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 2/20\n",
      "12557/12557 [==============================] - 323s 26ms/step - loss: 8.0611 - acc: 0.0055 - val_loss: 8.2240 - val_acc: 0.0051\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.24858 to 8.22400, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 3/20\n",
      "12557/12557 [==============================] - 322s 26ms/step - loss: 7.9390 - acc: 0.0072 - val_loss: 8.1821 - val_acc: 0.0076\n",
      "\n",
      "Epoch 00003: val_loss improved from 8.22400 to 8.18206, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 4/20\n",
      "12557/12557 [==============================] - 337s 27ms/step - loss: 7.8685 - acc: 0.0087 - val_loss: 8.1639 - val_acc: 0.0102\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.18206 to 8.16391, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 5/20\n",
      "12557/12557 [==============================] - 339s 27ms/step - loss: 7.8061 - acc: 0.0092 - val_loss: 8.1439 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.16391 to 8.14390, saving model to saved_models/weights.best.hdf5\n",
      "Epoch 6/20\n",
      "12557/12557 [==============================] - 315s 25ms/step - loss: 7.7713 - acc: 0.0088 - val_loss: 8.1984 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 8.14390\n",
      "Epoch 7/20\n",
      "12557/12557 [==============================] - 311s 25ms/step - loss: 7.7563 - acc: 0.0089 - val_loss: 8.1857 - val_acc: 0.0089\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 8.14390\n",
      "Epoch 8/20\n",
      "12557/12557 [==============================] - 301s 24ms/step - loss: 7.7510 - acc: 0.0090 - val_loss: 8.1936 - val_acc: 0.0115\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 8.14390\n",
      "Epoch 9/20\n",
      "12557/12557 [==============================] - 294s 23ms/step - loss: 7.7555 - acc: 0.0096 - val_loss: 8.2352 - val_acc: 0.0121\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 8.14390\n",
      "Epoch 10/20\n",
      "12557/12557 [==============================] - 296s 24ms/step - loss: 7.7665 - acc: 0.0102 - val_loss: 8.2576 - val_acc: 0.0102\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 8.14390\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a980ef0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "model.fit(sub_train_set, sub_train_labels, \n",
    "          validation_data=(valid_set, valid_labels),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer, early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1570/1570 [==============================] - 11s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.267160603954533, 0.016560509554140127]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(test_set, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1570/1570 [==============================] - 11s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.246530307478206, 0.012101910828025478]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "model.evaluate(test_set, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
